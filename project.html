<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Human-Machine Interaction @ IIIT-D</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700,900" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="css/bootstrap-datepicker.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">

  </head>
  <body data-spy="scroll" data-target=".site-navbar-target" data-offset="300">

  	<style>
		.wrapper {
		    text-align: center;
		}

    .new_logo_des{
      display: inline-block;
      
    }
</style>

  <div class="site-wrap">

    <div class="site-mobile-menu site-navbar-target">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div>

    <div class="border-bottom top-bar py-2 bg-dark" id="home-section">
      <div class="container">
        <div class="row">
          <div class="col-md-6">
            <p class="mb-0">
              <span class="mr-3"><strong class="text-white">Phone:</strong> <a href="tel://#">+91-11-26907523</a></span>
              <span><strong class="text-white">Email:</strong> <a href="#">hmi(at)iiitd(dot)ac(dot)in</a></span>
            </p>
          </div>
          <div class="col-md-6">
            <ul class="social-media">
              <li><a href="https://www.facebook.com/hmi.iiitd/" class="p-2" target="_blank"><span class="icon-facebook"></span></a></li>
              <li><a href="https://twitter.com/hmi_iiitd" class="p-2" target="_blank"><span class="icon-twitter"></span></a></li>
              <li><a href="https://www.instagram.com/hmi_iiitd/" class="p-2" target="_blank"><span class="icon-instagram"></span></a></li>
              <li><a href="https://www.linkedin.com/company/human-machine-interaction/about/" class="p-2" target="_blank"><span class="icon-linkedin"></span></a></li>
              <!-- <li><a href="#" class="p-2"><i class="fa fa-globe" aria-hidden="true"></i></a></li> -->

            </ul>
          </div>
        </div>
      </div>
    </div>

    <header class="site-navbar py-4 bg-white js-sticky-header site-navbar-target" role="banner">

      <div class="container">
        <div class="row align-items-center">

          <div class="col-11 col-xl-2">
<!--             <h1 class="mb-0 site-logo"><a href="index.html" class="text-black h2 mb-0">Create<span class="text-primary">.</span> </a></h1>
 -->            <a class="mb-0 site-logo" href="index.html"><img class="new_logo_des" src="images/logo/logo_hit.jpg" id="header-logo" class="img-fluid" ></a>
          </div>

          <div class="col-12 col-md-10 d-none d-xl-block">
            <nav class="site-navigation position-relative text-right" role="navigation">

              <ul class="site-menu main-menu js-clone-nav mr-auto d-none d-lg-block">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li class="has-children">
                  <a href="project.html" class="nav-link">Research</a>
                  <ul class="dropdown">
                    <li><a href="project.html#projects-section">Projects</a></li>
                    <li><a class="nav-link" href="publication.html">Publications</a></li>
                  </ul>
                </li>

               <li class="has-children">
                  <a href="team.html" class="nav-link">Our Team</a>
                  <ul class="dropdown">

                <li><a href="team.html#director-section">Director</a></li>
                    <li><a href="team.html#members-section">Members</a></li>
                    <li><a href="team.html#alumni-section">Alumni</a></li>
                    <li><a href="team.html#collaborator-section">Collaborators</a></li>
                  </ul>
                </li>

                <li class="has-children">
                  <a href="resources.html" class="nav-link">Resources</a>
                  <ul class="dropdown">

                <li><a href="faq.html" class="nav-link">FAQ</a></li>
                <li><a href="blog.html" class="nav-link">Blogs</a></li>
                    <li><a href="resources.html#internat-res-section">Internal Resources</a></li>
                    <li><a href="resources.html#external-res-section">External Resources</a></li>
                    
                  </ul>
                </li>
                
                
                <li><a href="updates.html" class="nav-link">Updates</a></li>
                <!-- <li><a href="#-section" class="nav-link">Contact Us</a></li> -->
                <li class="has-children">
                  <a href="contact_us.html" class="nav-link">Let's Connect</a>
                  <ul class="dropdown">

                    <li><a href="join_us.html">Join Us</a></li>
                    <li><a href="contact_us.html">Contact Us</a></li>
                    
                  </ul>
                </li>
              </ul>
            </nav>
          </div>

          <div class="d-inline-block d-xl-none ml-md-0 mr-auto py-3" style="position: relative; top: 3px;"><a href="#" class="site-menu-toggle js-menu-toggle text-black"><span class="icon-menu h3"></span></a></div>

        </div>
      </div>

    </header>



    <div class="site-blocks-cover overlay" style="background-image: url(images/header/project_3.jpg);" data-aos="fade" data-stellar-background-ratio="0.5">
     <!-- Source of Image : https://upload.wikimedia.org/wikipedia/commons/d/d7/Research_Scene_Vector.svg -->
      <div class="container">
        <div class="row align-items-center justify-content-center text-center">
          <div class="col-md-12" data-aos="fade-up" data-aos-delay="400">

            <div class="row justify-content-center mb-4">
              <div class="col-md-12 text-center">
                <h1><strong>Projects</strong></h1>
                <!-- <p class="lead mb-5">Free Web Template by <a href="#" target="_blank">Colorlib</a></p> -->
                <!-- <div><a data-fancybox data-ratio="2" href="https://vimeo.com/317571768" class="btn btn-primary btn-md">Watch Video</a></div> -->
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>

    <section class="site-section" id="projects-section">
      <div class="container">
        
        <div class="row mb-5 justify-content-center">
          <div class="col-md-8 text-center">
            <h2 class="text-black h1 site-section-heading text-center">Projects</h2>
            <p class="lead"></p>
          </div>  
        </div>

        <div class="row mb-5 justify-content-center">
          <div id="myBtnContainer">
            <button class="btn_pr active" onclick="filterSelection('all')">Show all</button>
            <button class="btn_pr" onclick="filterSelection('UbiquitousComputing')">Ubiquitous Computing</button>
            <button class="btn_pr" onclick="filterSelection('hri')">HRI</button>
            <button class="btn_pr" onclick="filterSelection('affcom')">Affective Computing</button>
          </div>
        </div>
      
      </div>
    

      <div class="container-fluid">
        <div class="row">
      
        <!-- <div class="filterDiv A"> -->
          <div class="col-md-6 col-lg-4 filterDiv UbiquitousComputing">
                <!-- <a class="media-1" data-fancybox data-src="#Driver-VR-content" href="javascript:;"> -->
                <a class="media-1" href="himanshu_proj.html" target="_blank"> 
                  <img src="images/project_thumb_2/p8.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Analyzing the Cognitive Driving Behavior of Indian Drivers in Virtual Environment</h2> -->
                    <!-- <span class="category">Virtual Reality, Affective Computing</span> -->
                  </div>
                </a>
          </div>
        <!-- </div> -->

        <!-- <div class="filterDiv A"> -->
          <div class="col-md-6 col-lg-4 filterDiv affcom">
            <!-- <a class="media-1" data-fancybox data-src="#emotion-in-movie-content" href="javascript:;"> -->
            <a class="media-1" href="aarushi_proj.html" target="_blank">  
              <img src="images/project_thumb_2/p9.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                <div class="media-1-content">
                    <!-- <h2>Recognizing Induced Emotions of Movie Audiences: Are Induced and Perceived Emotions the Same?</h2> -->
                    <!-- <span class="category">Affective Computing</span> -->
                </div>
            </a>
          </div>
        <!-- </div> -->

        <!-- <div class="filterDiv A"> -->
          
          <div class="col-md-6 col-lg-4 filterDiv hri">
                <!-- <a class="media-1" data-fancybox data-src="#social-robot-primary-edu-content" href="div_proj_template.html"> -->
                  <a class="media-1" href="div_proj_template.html" target="_blank">
                    <img src="images/project_thumb_2/p10.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                    <div class="media-1-content">
                      <!-- <h2>Social Robot for Primary Education</h2> -->
                      <!-- <span class="category">Accepted at: HRI'20</span> -->
                    </div>
                  </a>
          </div>
        
        <!-- </div> -->

        <!-- <div class="filterDiv A"> -->
          <div class="col-md-6 col-lg-4 filterDiv affcom">
            <!-- <a class="media-1" data-fancybox data-src="#Vyaktitv-content" href="javascript:;"> -->
            <a class="media-1" href="vyaktitv_shahid.html" target="_blank"> 
              <img src="images/project_thumb_2/p2.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                <div class="media-1-content">
                    <!-- <h2>Vyaktitv: Multimodal Personality Assesment Peer-to-Peer Hindi Conversation</h2> -->
                    <!-- <span class="category">Speech Processing,Affective Computing</span> -->
                </div>
            </a>
          </div>
        <!-- </div> -->

        <!-- <div class="filterDiv A"> -->
          <div class="col-md-6 col-lg-4 filterDiv Ubiquit">
                <!-- <a class="media-1" data-fancybox data-src="#adhd-proj-content" href="javascript:;"> -->
                <a class="media-1" href="adhd_proj.html" target="_blank">   
                  <img src="images/project_thumb_2/p1.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 3px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Engagement Analysis on Children with ADHD</h2> -->
                    <!-- <span class="category">Affective Computing</span> -->
                  </div>
                </a>
          </div>
        <!-- </div> -->

        <!-- <div class="filterDiv B"> -->
          <div class="col-md-6 col-lg-4 filterDiv hri">
                <!-- <a class="media-1" data-fancybox data-src="#hri-asd-content" href="javascript:;"> -->
                <a class="media-1" href="hri_asd.html" target="_blank">   
                  <img src="images/project_thumb_2/p11.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>HRI for ASD Diagnosis</h2> -->
                    <!-- <span class="category">Affective Computing, Human-Robot Interaction</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv hri">
                <!-- <a class="media-1" data-fancybox data-src="#hri-content" href="javascript:;"> -->
                <a class="media-1" href="hri_content.html" target="_blank">  
                  <img src="images/project_thumb_2/p12.jpg" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>HRI in public settings in India</h2> -->
                    <!-- <span class="category">Human-Robot Interaction</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv UbiquitousComputing">
                <!-- <a class="media-1" data-fancybox data-src="#twitter-content" href="javascript:;"> -->
                <a class="media-1" href="twitter_content.html" target="_blank">   
                  <img src="images/project_thumb_2/p3.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>An Investigation of Fortune 100 Companies: Insights to improve Twitter Engagement</h2> -->
                    <!-- <span class="category">Ubiquitous Computing</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv affcom">
                <!-- <a class="media-1" data-fancybox data-src="#depfuse-content" href="javascript:;"> -->
                <a class="media-1" href="depfuse_content.html" target="_blank">  
                  <img src="images/project_thumb_2/p4.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>DepFuseNet: Depression Recognition from Audio-Visual Features Using Multi-Modal Fusion</h2> -->
                    <!-- <span class="category">Multimodal Computing</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv affcom">
                <!-- <a class="media-1" data-fancybox data-src="#mihir-content" href="javascript:;"> -->
                <a class="media-1" href="mihir_content.html" target="_blank">   
                  <img src="images/project_thumb_2/p5.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Feature Selection for Clickbait Detection</h2> -->
                    <!-- <span class="category">Deep Learning</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv hri">
                <!-- <a class="media-1" data-fancybox data-src="#anmol-mihir-content" href="javascript:;"> -->
                <a class="media-1" href="anmol_mihir_content.html" target="_blank">   
                  <img src="images/project_thumb_2/p6.jpg" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Smart Human Robot Interaction</h2> -->
                    <!-- <span class="category">Human Robot Interaction</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv UbiquitousComputing">
                <!-- <a class="media-1" data-fancybox data-src="#anmol-eye-content" href="javascript:;"> -->
                <a class="media-1" href="anmol_eye.html" target="_blank">  
                  <img src="images/project_thumb_2/p7.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Engagement Evaluation in MOOC Environments using Gaze Estimation</h2> -->
                    <!-- <span class="category">Affective Computing</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv affcom">
                <!-- <a class="media-1" data-fancybox data-src="#shubhangi-ashwini-content" href="javascript:;"> -->
                <a class="media-1" href="shubhangi_proj.html" target="_blank">
                  <img src="images/project_thumb_2/p14.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Facial Actions for Artificial Agents</h2> -->
                    <!-- <span class="category">Human Robot Interaction</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv UbiquitousComputing">
                <!-- <a class="media-1" data-fancybox data-src="#chirag-content" href="javascript:;"> -->
                <a class="media-1" href="chirag_content.html" target="_blank">
                  <img src="images/project_thumb_2/p15.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Parrot: Picture-Based App for Verbal Communication</h2> -->
                    <!-- <span class="category">Ubiquitous Computing</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv affcom">
                <!-- <a class="media-1" data-fancybox data-src="#devashi-content" href="javascript:;"> -->
                <a class="media-1" href="devashi_content.html" target="_blank" >
                  <img src="images/project_thumb_2/p16.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Feature Extraction and Feature Selection for Emotion Recognition using Facial Expression</h2> -->
                    <!-- <span class="category">Affective Computing</span> -->
                  </div>
                </a>
          </div>

          <div class="col-md-6 col-lg-4 filterDiv affcom">
                <!-- <a class="media-1" data-fancybox data-src="#devashi-content" href="javascript:;"> -->
                <a class="media-1" href="anxiety_in_adults.html" target="_blank" >
                  <img src="images/project_thumb_2/p13.png" alt="Image" class="img-fluid" height="683.76" width="683.76" style="padding: 2px; border: 2px solid #000000;">
                  <div class="media-1-content">
                    <!-- <h2>Predicting Anxiety in Adults Using Physiological Signals</h2> -->
                    <!-- <span class="category">Affective Computing</span> -->
                  </div>
                </a>
          </div>
        <!-- </div> -->



      <div style="display: none;" id="hri-asd-content">

        <div class="container">
          <div class="row justify-content-center mb-5">
            <div class="text-center pb-1">
              <h3 class="text-black h1 site-section-heading">HRI for early ASD Diagnosis in Children</h3>
              <img src="images/resized_image/shahid_proj.jpg" alt="Image" class="img-fluid rounded" >
            </div>
          </div>

          <div class="row mb-5">
            <div class="col-md-12 order-md-1" data-aos="fade">
              <div class="col-17 mb-4">
                  <p class="text-primary lead" > <strong>Objective</strong></p>
                  <p class="lead" style="text-align:justify;">Autism is a developmental disorder under the broad spectrum of pervasive developmental disorders characterized by impairment in social interaction, communication skills, and repetitive and restricted behavior of the individual. The children diagnosed with ASD specifically benefit from early interventions, ideally between the ages one to five, as these interventions are designed to take advantage of the learning potential that the brain of a young child possesses. Hence early diagnosis of ASD is crucial in the development of these children. There is no biological markers for ASD and the diagnosis of ASD relies heavily on the behavioural observations made by the expert clinicians combined with assessments based on parent responses on the behavioural history of the child since. The intricacies of the diagnostic procedure often led to misjudgment of the condition. Hence, robot-assisted diagnosis systems can be employed to improve the early detection of ASD in an automated assessment manner.  We aim to create a model that can track child behaviour, quantify their state and assist the therapist in the diagnosis of the child. Multi-modal data collection and analysis will be performed for the child behaviour assessment and diagnosis. Our aim is to replace the labour intensive diagnostic procedure with  a more objective and effective one with the help of social robots.</p>
                  <br>


                  <p class="text-primary lead" > <strong>Team members</strong></p>
                  <p class="lead" style="text-align:justify;">
                    Ashwini B<br>
                    Ananya Bhatia
                  </p>
                  <br>

                  <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                  <p class="lead" style="text-align:justify;">
                    <h6 class="text-primary lead">Ashwini B</h6>
                    <ul>
                      <li>Task design for Diagnosis</li>
                      <li>Data Collection for multimodal analysis</li>
                      <li>Facial Emotion Recognition</li>
                    </ul>
                    
                      
                    <h6 class="text-primary lead">Ananya Bhatia</h6>
                    <ul>
                      <li>Gaze Detection and Model development for diagnostic task administration</li>
                    </ul>  

                  </p>
                  <br>
                  <!-- <p class="text-primary lead" > <strong>Publication</strong></p>
                  <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p>
    -->
              </div>
            </div>
          </div>
        </div>
      </div>      



      <div style="display: none;" id="hri-content">

        <div class="container">
          <div class="row justify-content-center mb-5">
            <div class="text-center pb-1">
              <h3 class="text-black h1 site-section-heading">HRI in Public Settings</h3>
              <img src="images/hri_proj.png" alt="Image" class="img-fluid rounded" >
            </div>
          </div>

          <div class="row mb-5">
            <div class="col-md-12 order-md-1" data-aos="fade">
              <div class="col-17 mb-4">
                <p class="text-primary lead" > <strong>Objective</strong></p>
                  <p class="lead" style="text-align:justify;">There have been several studies in the field of Human-Robot Interaction, specifically regarding introduction of social humanoid robots in public places for various applications and purposes. These robots are used in research, education, and healthcare all over the world. In order to serve these different purposes, robot require advanced dialog and interaction capabilities to help its users. This project aims to study the social dimension of human interaction with social robots. To the best of my knowledge, previous studies have not taken the Indian population into context. So, the social dimension will be studied and explored within the Indian population. Through the results obtained I also aim to make use of this study to develop better HRI for a wide range of applications. I hope this research can serve as the ground truth, can be used for reference and as pointers for guidelines to design interactions which much more closely mimic human expectations.
                  </p>
                  <br>

                  <!-- <p class="text-primary lead" > <strong>Publication</strong></p>
                  <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p>
    -->
              </div>
            </div>
          </div>
        </div>
      </div>


      <div style="display: none;" id="twitter-content">

        <div class="container">
          <div class="row justify-content-center mb-5">
            <div class="text-center pb-1">
              <h3 class="text-black h1 site-section-heading">Best Practices for Better Twitter Engagement: What small and medium-sized enterprises can learn from Top Fortune 100 companies</h3>
              <img src="images/tanya_kapur_proj.png" alt="Image" class="img-fluid rounded" >
            </div>
          </div>

          <div class="row mb-5">
            <div class="col-md-12 order-md-1" data-aos="fade">
              <div class="col-17 mb-4">
                <p class="text-primary lead" > <strong>Objective</strong></p>
                  <p class="lead" style="text-align:justify;">There have been several studies in the field of Human-Robot Interaction, specifically regarding introduction of social humanoid robots in public places for various applications and purposes. These robots are used in research, education, and healthcare all over the world. In order to serve these different purposes, robot require advanced dialog and interaction capabilities to help its users. This project aims to study the social dimension of human interaction with social robots. To the best of my knowledge, previous studies have not taken the Indian population into context. So, the social dimension will be studied and explored within the Indian population. Through the results obtained I also aim to make use of this study to develop better HRI for a wide range of applications. I hope this research can serve as the ground truth, can be used for reference and as pointers for guidelines to design interactions which much more closely mimic human expectations.
                  </p>
                  <br>

                  <p class="text-primary lead" > <strong>Team members</strong></p>
                  <p class="lead" style="text-align:justify;">
                    Tanya Kapur<br>
                    Anvit Sachadev
                  </p>
                  <br>

                  <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                  <p class="lead" style="text-align:justify;">
                    <h6 class="text-primary lead">Tanya Kapur</h6>
                    <ul>
                      <li>Scripts for data analysis</li>
                      <li>Analysis of the data</li>
                      <li>Coming up with useful insights and recommendations</li>
                      <li>Overall Project Management</li>
                      <li>Research Paper Writing</li>
                    </ul>
                    
                      
                    <h6 class="text-primary lead">Tanya Kapur</h6>
                    <ul>
                      <li>Data Collection scripts</li>
                      <li>Collecting Data</li>
                      <li>Data pre-processing</li>
                      <!-- <li>Overall Project Management</li> -->
                      <!-- <li>Research Paper Writing</li> -->
                    </ul>  

                  </p>
                  <br>

                  <!-- <p class="text-primary lead" > <strong>Publication</strong></p>
                  <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p>
    -->
              </div>
            </div>
          </div>
        </div>
      </div>      


      <div style="display: none;" id="Vyaktitv-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Vyaktitv: Multimodal Personality Assessment in Peer-to-Peer Hindi Conversations</h3>
                  <img src="images/resized_image/shahid_proj.jpg" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Objective</strong></p>

                        <p class="lead" style="text-align:justify;">Automatically detecting and analyzing personality traits can aid in several other applications in domains like mental health recognition and human resource management. The primary limitation of a wide range of techniques used for personality prediction so far is that they analyze these traits for each individual in isolation. In contrast, personality is intimately linked with our social behavior. In this work,we conduct an experiment where the subjects participated in peer-to-peer conversations in Hindi. To the best of our knowledge, no work has been done on analyzing personality in such a setting.Our contributions include the first peer-to-peer Hindi conversation-based dataset for personality prediction, Vyaktitv, which consists of high-quality audio and video recordings of the participants, along with Hinglish-based textual transcriptions for each conversation. The dataset also contains a rich set of socio-demographic features, including gender, age, income, and several others, for all the participants. We release the dataset for public use, along with a preliminary multimodal analysis of the Big Five personality traits based on audio, video, and linguistic features.</p>
                        <br>

                        <p class="text-primary lead" > <strong>Student Collaborator</strong></p> <p class="lead" style="text-align: justify;"> Shahid Nawaz Khan (IIIT Delhi)</p>
                        <p class="text-primary lead" > <strong>Faculty Collaborator</strong></p> <p class="lead" style="text-align: justify;"> Dr. Rajiv Ratn Shah (IIIT Delhi)</p>
                      

                        <!-- <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p>
 -->
                  </div>           
                </div>
              </div>

          </div>
        </div>      





          <div style="display: none;" id="social-robot-primary-edu-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Social Robot for Primary Education</h3>
                  <img src="images/div_proj_1.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">Research focused upon Child-Robot Interaction shows that robots in the classroom can support diverse learning goals amongst pre-school children. However, studies with children and robots in the Global South are currently limited. To address this gap, we conducted a study with children aged 4-8 years at a community school in New Delhi, India, to understand their interaction and experiences with a social robot. The children were asked to teach the English alphabet to a Cozmo robot using flash cards. Preliminary findings suggest that the children orient to the robot in a variety of ways including as a toy or pet. These orientations need to be explored further within the context of the Global South.</p>
                        <br>

                        <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p>

                  </div>           
                </div>
              </div>

          </div>
        </div>


        <div style="display: none;" id="Driver-VR-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Analyzing the Cognitive Driving Behavior of Indian Drivers in Virtual Environment</h3>
                  <img src="images/himanshu_proj_2.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">A driver has to be very attentive while driving. He/she has to always stay calm. But during driving there can be various stressful instances which can affect the cognitive behavior of the driver and can lead to fatal accidents. In the coming age of autonomous driving the vehicles can assist the driver during these stressful conditions which can improve the driving experience. For this we need to find these stressful scenarios and analyse the cognitive behavior of the driver in these scenarios. Cognitive behavior of an individual can be analysed with the help of EEG or skin conductance signals. We analyse various parameters of EEG and skin conductance signal to states like stress, attention, drowsiness etc.</p>
                        <br>

                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Himanshu Bansal<br>
                        Pramil Panjawani<br>
                        Satvika Anand<br>
                      </p>
                      <br>

                      <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                      <p class="lead" style="text-align:justify;">
                        <h6 class="text-primary lead">Himanshu Bansal</h6>
                        <ul>
                          <li>VR development</li>
                          <li>User Study</li>
                          <li>Compiling the results</li>
                          <li>EEG</li>
                        </ul>

                        <h6 class="text-primary lead">Pramil Panjawani</h6>
                        <ul>
                          <li>Skin Conductance analysis</li>
                        </ul>

                        <h6 class="text-primary lead">Satvika Anand</h6>
                        <ul>
                          <li>EEG analysis</li>
                        </ul>


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>


        <div style="display: none;" id="depfuse-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">DepFuseNet: Depression Recognition from Audio-Visual Features using Multi-modal Fusion</h3>
                  <img src="images/neha_proj.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">Depression is a common and serious mental disorder that affects humans all over the world, therefore requires efficient assistance in its early recognition. Previous research has studied several machine learning and deep learning methods for depression recognition. However, a lack of significant efficiency and objectivity, makes these methods less utile. In this paper, we propose an efficient multimodal deep learning approach for depression recognition using audio-visual modalities. We performed feature extraction followed by feature selection on the publicly available DAIC-WOZ database, which contains the raw audios and video features for the clinical interviews of 56 patients with depression and 133 individuals without depression. We used openSmile toolkit to extract 4 different types of audio features, Chroma, PLP, MFCC, and Prosodic. Due to the privacy concerns, clinical interview videos were not available, and hence, we employed the baseline video features provided with the database, namely 2D/3D facial points, HOG, Action Units, Gaze directions, and Head Pose. We further performed feature selection on these audio-visual features to identify the most significant ones for depression recognition. Additionally, we developed a concatenated fusion-based Long Short-Term Memory (LSTM) architecture, named DepFuseNet which utilizes the identified significant features. The DepFuseNet receives insights from both the modalities and also, the handcrafted traditional features reduce the computational complexity of the DepFuseNet architecture. The results obtained from the proposed architecture DepFuseNet outperforms the state of the art method for depression recognition, which affirms the effectiveness of the proposed method for depression recognition using audio/visual modalities.</p>
                        <br>

                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Neha Goyal<br>
                        Devashi Choudhary<br>
                      </p>
                      <br>

                      <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                      <p class="lead" style="text-align:justify;">
                        <h6 class="text-primary lead">Neha Goyal</h6>
                        <!-- <ul> -->
                        <!-- </ul> -->

                        <h6 class="text-primary lead">Devashi Choudhary</h6>
                        <!-- <ul> -->
                          <!-- <li>Skin Conductance analysis</li> -->
                        <!-- </ul> -->


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>


        <div style="display: none;" id="mihir-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Feature Selection for Clickbait Detection</h3>
                  <img src="images/mihir_proj.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">Clickbaits are posts that aim to exploit the natural curiosity of humans by providing incomplete or incorrect information to get users to visit the full posts that typically consist of sensationalized or misleading information. The information provided is just enough to incite their interest. Among various attempts to identify and curb the practice of clickbaits, it can be useful to identify a highly successful method. A clickbait-free environment will help users have a better time browsing the internet. We draw inspiration from the Clickbait Challenge of 2017 in which multiple approaches were proposed to detect clickbaits using machine learning and deep learning techniques. Within these approaches, we find a plethora of features that were used in the models. To the best of our knowledge, a systematic study of these features and their correlation with each other has never been done before. We aim to identify a trade-off between the number of features and the performance of the model to facilitate faster processing without losing much performance. With this knowledge, a more accurate and faster clickbait model can potentially be deployed that would help improve user experience online in real-time. It can also facilitate better research opportunities in the clickbait detection domain.</p>
                        <br>

                      <!-- <p class="text-primary lead" > <strong>Team members</strong></p> -->
                      <!-- <p class="lead" style="text-align:justify;"> -->
                        <!-- Neha Goyal<br> -->
                        <!-- Devashi Choudhary<br> -->
                      <!-- </p> -->
                      <!-- <br> -->

                      <!-- <p class="text-primary lead" > <strong>Team member contributions</strong></p> -->
                      <!-- <p class="lead" style="text-align:justify;"> -->
                        <!-- <h6 class="text-primary lead">Neha Goyal</h6> -->
                        <!-- <ul> -->
                        <!-- </ul> -->

                        <!-- <h6 class="text-primary lead">Devashi Choudhary</h6> -->
                        <!-- <ul> -->
                          <!-- <li>Skin Conductance analysis</li> -->
                        <!-- </ul> -->


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>

        <div style="display: none;" id="anmol-mihir-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Smart Human Robot Interaction</h3>
                  <img src="images/anmol_mihir_proj.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">With the world driving towards automation and technological advancements, the utilisation of service robots for smart home environments is gaining limelight. However, to be able to perform efficiently in a human-like manner, robots need to integrate tasks like object detection and localization with semantic knowledge representation and activity recognition. Therefore, researchers are increasingly working towards mapping high-level knowledge of the world to the robotâ€™s understanding to produce the desired outcome. The goal is to make the robot capable of making intelligent decisions and carry out its job without human intervention as far as possible. 
                        Our work involves making a robot predict activities of daily living of a person living in a smart home environment. Based on the routine of the person, the predictions made by the robot can be useful in making viable choices and adhere to his/her needs. We train an ensemble deep learning architecture to ascertain the need for some robot action and further determine the type of action required. To the best of our knowledge, predicting robot actions based on the semantic nature of the environment using Deep Learning has not been carried out before. </p>
                        <br>

                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Anmol Singhal <br>
                        Mihir Goyal <br>
                      </p>
                      <br>

                      <!-- <p class="text-primary lead" > <strong>Team member contributions</strong></p> -->
                      <!-- <p class="lead" style="text-align:justify;"> -->
                        <!-- <h6 class="text-primary lead">Neha Goyal</h6> -->
                        <!-- <ul> -->
                        <!-- </ul> -->

                        <!-- <h6 class="text-primary lead">Devashi Choudhary</h6> -->
                        <!-- <ul> -->
                          <!-- <li>Skin Conductance analysis</li> -->
                        <!-- </ul> -->


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>

        <div style="display: none;" id="anmol-eye-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Engagement Evaluation in MOOC Environments using Gaze Estimation</h3>
                  <img src="images/anmol_eye_proj.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">With the tremendous increment in reach of the internet, every aspect of communication in our lives are slowly turning into digital modes of communication. Massive Open Online Courses (MOOCs) allow the learners imbibe quality learning at their own comfort through form of digital learning. MOOCs check the quality of learning among their users by multiple subjective and objective tests that ensure quality learning by the users, but these results can often hoodwink the teachers since these quizzes can be given by other accounts and know the answers beforehand. We propose a network that requires no extra hardware, and low computation power , that can help users as well as the teacher estimate the engagement levels of the learner while consuming the resources. Our network uses features of the user's eyes as well as face, combined with the salient features extracted from the MOOC video to estimate attention. This can help learners learn more efficiently, as it can alert the user when their engagement levels are low, as well as let the teacher know so that they can improve on the sections of their lectures where the learners are losing their interests.</p>
                        <br>

                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Anmol Prasad <br>
                        Saksham Vohra <br>
                      </p>
                      <br>

                      <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                      <p class="lead" style="text-align:justify;">
                        <h6 class="text-primary lead">Anmol Prasad</h6>
                        <ul>
                          <li>Gaze tracking program</li>
                          <li>Data collection protocol</li>
                          <li>Data pre processing</li>
                        </ul>

                        <h6 class="text-primary lead">Saksham Vohra</h6>
                        <ul>
                          <li>Gaze tracking program</li>
                          <li>Data collection protocol</li>
                          <li>Deep Learning architecture</li>
                        </ul>


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>

        <div style="display: none;" id="shubhangi-ashwini-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Facial Actions for Artificial Agents</h3>
                  <img src="images/shubhangi_ashwini_proj.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">Autism spectrum disorder (ASD) is a developmental disorder that affects communication and behavior. The diagnostic tasks are often complex and cumbersome due to the mental health conditions which children with ASD suffer from, some of which are higher levels of anxiety, depression, attention deficit hyperactivity disorder (ADHD), and disruptive behavior disorders. In this work, we investigate how emotion recognition can be leveraged to achieve sustained attention during the diagnostic tasks in children with autism. We focus on capturing the emotions of the child during diagnostic tasks and the corresponding facial signals that the agent has to display during diagnostic task administration eventually leading to the successful completion of these diagnostic tasks and better diagnosis.</p>
                        <br>

                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Shubhangi Butta <br>
                        Ashwini <br>
                      </p>
                      <br>

                      <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                      <p class="lead" style="text-align:justify;">
                        <h6 class="text-primary lead">Shubhangi Butta</h6>
                        <ul>
                          <!-- <li>Gaze tracking program</li> -->
                          <!-- <li>Data collection protocol</li> -->
                          <!-- <li>Data pre processing</li> -->
                        </ul>

                        <h6 class="text-primary lead">Ashwini</h6>
                        <ul>
                          <!-- <li>Gaze tracking program</li> -->
                          <!-- <li>Data collection protocol</li> -->
                          <!-- <li>Deep Learning architecture</li> -->
                        </ul>


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>

        <div style="display: none;" id="chirag-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Parrot: Picture-Based App for Verbal Communication</h3>
                  <img src="images/chirag_proj.png" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">The aim of the project is to develop a mobile/tablet app to facilitate verbal communication for children with Autism Spectrum Disorder (ASD). The app will be based on a picture exchange communication system (PECS) and will work as a communication aid for children with ASD and for their parents and/or caregivers.</p>
                        <br>

                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Chirag Jain <br>
                        Bhavika Rana <br>
                      </p>
                      <br>

                      <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                      <p class="lead" style="text-align:justify;">
                        <h6 class="text-primary lead">Chirag Jain</h6>
                        <ul>
                          <li>App Developer</li>
                        </ul>

                        <h6 class="text-primary lead">Bhavika Rana</h6>
                        <ul>
                          <li>UI/UX Designer</li>
                        </ul>


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>

        <div style="display: none;" id="devashi-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Feature Extraction and Feature Selection for Emotion Recognition using Facial Expression</h3>
                  <img src="images/devashi_proj.jpg" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>
                        <p class="lead" style="text-align:justify;">Facial expressions play a signiï¬cant role in describing the emotions of a person. Due to its applicability to a wide range of applications, such as human-computer interaction, driver status monitoring, etc., Facial Expression Recognition (FER) has received substantial attention among the researchers. According to the earlier studies, a small feature set is used for the extraction of facial features for FER system. To date, a systematic comparison of the facial features does not exist. Therefore, in the current research, we identiï¬ed 18 diï¬€erent facial features (cardinality of 46,352) by reviewing 25 studies and implemented them on the publicly available Extended-Cohn-Kanade (CK+) dataset. After extracting facial features, we performed Feature Selection (FS) using Joint Mutual Information (JMI), Conditional Mutual Information Maximization (CMIM) and MaxRelevance Min-Redundancy (MRMR) and explain the systematic comparison between them, and for classiï¬cation, we applied various machine learning techniques. The Bag of Visual Words (BoVW) model approach results in signiï¬cantly higher classiï¬cation accuracy over the formal approach. Also, we found that the optimal classiï¬cation accuracy for FER can be obtained by using only 20% of the total identiï¬ed features. Grey comatrix and haralick features were explored for the ï¬rst time for the FER and grey comatrix feature outperformed several most commonly used features Local Binary Pattern (LBP) and Active Appearance Model (AAM). Histogram of Gradients (HOG) turns out to be the most signiï¬cant feature for FER followed Local Directional Positional Pattern (LDSP) and grey comatrix. Using signiï¬cant features, we developed a guided convolution neural network (GCNN) having a reduced model complexity and an improved classiï¬cation accuracy for FER.</p>
                        <br>

                      <!-- <p class="text-primary lead" > <strong>Team members</strong></p> -->
                      <!-- <p class="lead" style="text-align:justify;"> -->
                        <!-- Chirag Jain <br> -->
                        <!-- Bhavika Rana <br> -->
                      <!-- </p> -->
                      <!-- <br> -->

                      <!-- <p class="text-primary lead" > <strong>Team member contributions</strong></p> -->
                      <!-- <p class="lead" style="text-align:justify;"> -->
                        <!-- <h6 class="text-primary lead">Chirag Jain</h6> -->
                        <!-- <ul> -->
                          <!-- <li>App Developer</li> -->
                        <!-- </ul> -->

                        <!-- <h6 class="text-primary lead">Bhavika Rana</h6> -->
                        <!-- <ul> -->
                          <!-- <li>UI/UX Designer</li> -->
                        <!-- </ul> -->


                    <!--     <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>





          <div style="display: none;" id="adhd-proj-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Engagement Analysis on children with ADHD </h3>
                  <img src="images/adhd_proj.jpg" alt="Image" class="img-fluid rounded" >
                </div>

              </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">Attention Deficit hyperactivity disorder (ADHD) is a medical condition portraying symptoms of aggression, hyperactivity, and an inattention. It prevails among 7.8% of Children all over the world. This project aims to measure the engagement of children with ADHD. In this study, the bio-signal parameters of an individual are recorded in an online one to one tutorial session with an educator. Previous studies have established modalities like EEG, Eye-tracking, GSR have shown significant results in investigating and monitoring ADHD related experiments.EEG research has attempted to characterize and quantify the neurophysiology of attention-deficit/hyperactivity disorder (ADHD), most consistently associating it with increased frontocentral theta band activity and increased theta to beta (Î¸/Î²) power ratio during rest compared to non-ADHD controls. Past studies have demonstrated EEG brainwave feedback as an efficient method for assessing attention. We are using EEG Emotiv EPOC+ for measuring the brain activity by 14 electrodes. The EEG device provides the data in the form of motion, performance data including stress, anxiety, signal strength, and contact quality of 14 electrodes. Eye movements can be informative of the underlying mechanisms of complex disorders like ADHD. Eye-tracking provides researchers with many benefits, often without requiring deep expertise to implement. It is a well-established method for measuring visual attention, cognitive â€œload,â€ goal pursuit, and implicit preferences. Variations in eye movements, including speed of movement, duration of fixations, patterns and frequency of blinks, and patterns of visual searching behavior, are all relevant to how a person is responding to a visual stimulus. We are using the Tobii 4c eye-tracker, which operates at a sampling frequency of 90 Hz. The third modality is the Galvanic Skin Response (GSR), which estimates the skin conductivity data, gyroscope data, accelerometer data, and PPG data. Shimmer 3 is the device used for GSR data collection and evaluation. The first phase involves data collection with synchronization, for which we have designed a protocol of 55 minutes comprising of a couple of 20 minutes sessions with the educator both preceded and followed by a 5 minutes relaxation session. The focus is to combine multiple modalities mainly for better evaluation of engagement using statistical machine learning and deep learning techniques.</p>
                        <br>


                      <p class="text-primary lead" > <strong>Team members</strong></p>
                      <p class="lead" style="text-align:justify;">
                        Harshit Chauhan<br>
                        Prachi Arora<br>
                        Satvika Anand<br>
                        Pramil Panjawani
                      </p>
                      <br>

                      <p class="text-primary lead" > <strong>Team member contributions</strong></p>
                      <p class="lead" style="text-align:justify;">
                        <h6 class="text-primary lead">Harshit Chauhan</h6>
                        <ul>
                          <li>Eye tracker and webcam data collection and analysis Application of multi modal deep learning model for combined analysis.</li>
                          <li>Application of multi modal deep learning model for combined analysis.</li>
                        </ul>

                        <h6 class="text-primary lead">Pramil Panjawani</h6>
                        <ul>
                          <li>GSR data collection and analysis</li>
                        </ul>

                        <h6 class="text-primary lead">Satvika Anand</h6>
                        <ul>
                          <li>EEG data collection and analysis</li>
                        </ul>  

                        <h6 class="text-primary lead">Prachi Arora</h6>
                        <ul>
                          <li>Eye tracking data collection and Analysis</li>
                        </ul>

                        


                       <!--  <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p> -->

                  </div>           
                </div>
              </div>

          </div>
        </div>  
    


          

        


      <div style="display: none;" id="emotion-in-movie-content">

            <div class="container">

              <div class="row justify-content-center mb-5">

                <div class="text-center pb-1">
                  <h3 class="text-black h1 site-section-heading">Recognizing Induced Emotions of Movie Audiences: Are Induced and Perceived Emotions the Same?</h3>
                  <img src="images/aarushi_proj.png" alt="Image" class="img-fluid rounded" >
                </div>
            </div>

       
              <div class="row mb-5">

                <div class="col-md-12 order-md-1" data-aos="fade">

                  <div class="col-17 mb-4">
                        <p class="text-primary lead" > <strong>Abstract</strong></p>

                        <p class="lead" style="text-align:justify;">The prediction of oneâ€™s emotional response to affective movie content has been a challenging task in affective computing. Previously, work has been done on the same in regards to Hollywood movies. However, the great diversity of Bollywood movies is often missed out from such projects. This paper will take into account specifically Bollywood content. Firstly, a database of Bollywood movie clips will be created by collection and annotation of perceived emotions via crowdsourcing. We will then collect data for induced emotions by using facial expression recognition tools. Along with facial expression, we will also collect data of EEG (Electroencephalogram) signals and GSR (Galvanic Skin Response) signals. Lastly, with the application of machine learning a model will be trained on half of the data recorded to predict the emotions of movie audiences in another half of the data</p>
                        <br>

                        <!-- <p class="text-primary lead" > <strong>Publication</strong></p>
                        <p class="lead" style="text-align:justify;">Kumar Singh, D., Sharma, S., Shukla, J., & Eden, G. (2020, March).<a href="https://dl.acm.org/doi/abs/10.1145/3371382.3378315"><strong> <i> Toy, Tutor, Peer, or Pet? Preliminary Findings from Child-Robot Interactions in a Community School.</i></strong></a> In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (pp. 325-327).<a href="https://www.youtube.com/watch?v=hMyGznAFYMk"><i><strong>[Video]</strong></i></a></p>
 -->
                  </div>           
                </div>
              </div>

          </div>
        </div>      

        

          


        </div>
      </div>
    </section>
      
    


    


<!-- 

    <section class="site-section">
      <div class="container">
        <div class="row">
          <div class="col-md-6 col-lg-4">
            <div class="p-3 box-with-humber">
              <div class="number-behind">01.</div>
              <h2 class="text-primary">Innovate</h2>
              <p class="mb-4">Lorem ipsum dolor sit amet consectetur adipisicing elit. Et praesentium eos nulla qui commodi consectetur beatae fugiat. Veniam iste rerum perferendis.</p>
              <ul class="list-unstyled ul-check primary">
                <li>Customer Experience</li>
                <li>Product Management</li>
                <li>Proof of Concept</li>
              </ul>
            </div>
          </div>

          <div class="col-md-6 col-lg-4">
            <div class="p-3 box-with-humber">
              <div class="number-behind">02.</div>
              <h2 class="text-primary">Create</h2>
              <p class="mb-4">Lorem ipsum dolor sit amet consectetur adipisicing elit. Et praesentium eos nulla qui commodi consectetur beatae fugiat. Veniam iste rerum perferendis.</p>
              <ul class="list-unstyled ul-check primary">
                <li>Web Design</li>
                <li>Branding</li>
                <li>Web &amp; App Development</li>
              </ul>
            </div>
          </div>

          <div class="col-md-6 col-lg-4">
            <div class="p-3 box-with-humber">
              <div class="number-behind">03.</div>
              <h2 class="text-primary">Scale</h2>
              <p class="mb-4">Lorem ipsum dolor sit amet consectetur adipisicing elit. Et praesentium eos nulla qui commodi consectetur beatae fugiat. Veniam iste rerum perferendis.</p>
              <ul class="list-unstyled ul-check primary">
                <li>Social Media</li>
                <li>Paid Campaigns</li>
                <li>Marketing &amp; SEO</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>
 -->

 <!-- 
    <section class="site-section" id="projects-section">
      <div class="container">
        <div class="row mb-5 justify-content-center">
          <div class="col-md-8 text-center">
            <h2 class="text-black h1 site-section-heading text-center">Projects</h2>
            <p class="lead">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Dolores, itaque neque, delectus odio iure explicabo.</p>
          </div>
        </div>
      </div>
      <div class="container-fluid">
        <div class="row">
          <div class="col-md-6 col-lg-4">
            <a href="images/img_1.jpg" class="media-1" data-fancybox="gallery">
              <img src="images/img_1.jpg" alt="Image" class="img-fluid">
              <div class="media-1-content">
                <h2>Bonzai Tree</h2>
                <span class="category">Web Application</span>
              </div>
            </a>
          </div>
          <div class="col-md-6 col-lg-4">
            <a href="images/img_2.jpg" class="media-1" data-fancybox="gallery">
              <img src="images/img_2.jpg" alt="Image" class="img-fluid">
              <div class="media-1-content">
                <h2>Simple Woman</h2>
                <span class="category">Branding</span>
              </div>
            </a>
          </div>
          <div class="col-md-6 col-lg-4">
            <a href="images/img_3.jpg" class="media-1" data-fancybox="gallery">
              <img src="images/img_3.jpg" alt="Image" class="img-fluid">
              <div class="media-1-content">
                <h2>Fruits</h2>
                <span class="category">Website</span>
              </div>
            </a>
          </div>

          <div class="col-md-6 col-lg-4">
            <a href="images/img_4.jpg" class="media-1" data-fancybox="gallery">
              <img src="images/img_4.jpg" alt="Image" class="img-fluid">
              <div class="media-1-content">
                <h2>Design Material</h2>
                <span class="category">Web Application</span>
              </div>
            </a>
          </div>
          <div class="col-md-6 col-lg-4">
            <a href="images/img_5.jpg" class="media-1" data-fancybox="gallery">
              <img src="images/img_5.jpg" alt="Image" class="img-fluid">
              <div class="media-1-content">
                <h2>Handy Food</h2>
                <span class="category">Branding</span>
              </div>
            </a>
          </div>
          <div class="col-md-6 col-lg-4">
            <a href="images/img_6.jpg" class="media-1" data-fancybox="gallery">
              <img src="images/img_6.jpg" alt="Image" class="img-fluid">
              <div class="media-1-content">
                <h2>Cat With Cup</h2>
                <span class="category">Website</span>
              </div>
            </a>
          </div>


        </div>
      </div>
    </section>

    <section class="section ft-feature-1">
      <div class="container">
        <div class="row align-items-stretch">
          <div class="col-12 bg-black w-100 ft-feature-1-content">
            <div class="row align-items-center">
              <div class="col-lg-5">

                  <img src="images/about_1.jpg" alt="Image" class="img-fluid mb-4 mb-lg-0">

              </div>
              <div class="col-lg-3 ml-auto">
                <div class="mb-5">
                  <h3 class="d-flex align-items-center"><span class="icon icon-beach_access mr-2"></span><span>Strategy</span></h3>
                  <p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Cumque ab nihil quam nesciunt.</p>
                  <p><a href="#">Read More</a></p>
                </div>

                <div>
                  <h3 class="d-flex align-items-center"><span class="icon icon-build mr-2"></span><span>Web Development</span></h3>
                  <p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Cumque ab nihil quam nesciunt.</p>
                  <p><a href="#">Read More</a></p>
                </div>

              </div>
              <div class="col-lg-3">
                <div class="mb-5">
                  <h3 class="d-flex align-items-center"><span class="icon icon-format_paint mr-2"></span><span>Art Direction</span></h3>
                  <p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Cumque ab nihil quam nesciunt.</p>
                  <p><a href="#">Read More</a></p>
                </div>

                <div>
                  <h3 class="d-flex align-items-center"><span class="icon icon-question_answer mr-2"></span><span>Copywriting</span></h3>
                  <p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Cumque ab nihil quam nesciunt.</p>
                  <p><a href="#">Read More</a></p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
 -->





<!-- 
    <section class="site-section bg-light" id="contact-section">
      <div class="container">
        <div class="row mb-5">
          <div class="col-12 text-center">
            <h2 class="text-black h1 site-section-heading">Contact Us</h2>
          </div>
        </div>
        <div class="row">
          <div class="col-md-7 mb-5">


            <form id="contact-form" method="post" action="contact-form-handler.php" class="p-5 bg-white">

              <h2 class="h4 text-black mb-5">Contact Form</h2>

              <div class="row form-group">
                <div class="col-md-12">
                  <label class="text-black" for="fname">Name</label>
                  <input type="text" id="fname" class="form-control">
                </div>
                <div class="col-md-6">
                  <label class="text-black" for="lname">Last Name</label>
                  <input type="text" id="lname" class="form-control">
                </div> 
              </div>

              <div class="row form-group">

                <div class="col-md-12">
                  <label class="text-black" for="email">Email</label>
                  <input type="email" id="email" class="form-control">
                </div>
              </div>

              <div class="row form-group">

                <div class="col-md-12">
                  <label class="text-black" for="subject">Subject</label>
                  <input type="subject" id="subject" class="form-control">
                </div>
              </div>

              <div class="row form-group">
                <div class="col-md-12">
                  <label class="text-black" for="message">Message</label>
                  <textarea name="message" id="message" cols="30" rows="7" class="form-control" placeholder="Write your notes or questions here..."></textarea>
                </div>
              </div>

              <div class="row form-group">
                <div class="col-md-12">
                  <input type="submit" value="Send Message" class="btn btn-primary btn-md text-white">
                </div>
              </div>


            </form>
          </div>
          <div class="col-md-5">

            <div class="p-4 mb-3 bg-white">
              <p class="mb-0 font-weight-bold">Address</p>
              <p class="mb-4" style="text-align:justify;">B-418, R & D Block, Indraprastha Institute of Information Technology, Delhi,
Okhla Industrial Estate, Phase III, (Near Govind Puri Metro Station), New Delhi - 110020, India</p>

              <p class="mb-0 font-weight-bold">Phone</p>
              <p class="mb-4"><a href="#">+91-11-26907523</a></p>

              <p class="mb-0 font-weight-bold">Email Address</p>
              <p class="mb-0"><a href="#">hmi@iiitd.ac.in</a></p>

            </div>

          </div>
        </div>
      </div>
    </section> -->

    <!-- <a href="#" class="bg-primary py-5 d-block">
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-md10"><h2 class="text-white">Let's Get Started</h2></div>
        </div>
      </div>
     </a>-->

 
    <footer class="site-footer">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <div class="row">
              <div class="col-md-5">
                <h2 class="footer-heading mb-4">About Us</h2>
                <p style="text-align: justify;">At Human Machine Interaction (HMI) lab, we investigate novel interaction mechanisms to empower human-machine cooperation. Our research is supported by the Intramural Start-up Grant from the <a href="https://www.iiitd.ac.in/">Indraprastha Institute of Information Technology-Delhi (IIIT-Delhi)</a>,<a href="https://cai.iiitd.ac.in/"> Infosys Center for Artificial Intelligence</a>, and <a href="https://cdnm.iiitd.ac.in/">TCS â€“ Centre for Design and New Media</a>.</p>
              </div>
              <div class="col-md-3 ml-auto">
                <h2 class="footer-heading mb-4">Features</h2>
                <ul class="list-unstyled">
                  <li><a href="index.html#about-section">About Us</a></li>
                  <li><a href="team.html">Team</a></li>
                  
                  <li><a href="contact_us.html">Contact Us</a></li>
                </ul>
              </div>
              <div class="col-md-3">
                <h2 class="footer-heading mb-4">Follow Us</h2>
                <a href="https://www.facebook.com/hmi.iiitd" class="pl-0 pr-3" target="_blank"><span class="icon-facebook"></span></a>
                <a href="https://twitter.com/hmi_iiitd" class="pl-3 pr-3" target="_blank"><span class="icon-twitter"></span></a>
                <a href="https://www.instagram.com/hmi_iiitd/" class="pl-3 pr-3" target="_blank"><span class="icon-instagram"></span></a>
                <a href="https://www.linkedin.com/company/human-machine-interaction/about/" class="pl-3 pr-3" target="_blank"><span class="icon-linkedin"></span></a>
              </div>
            </div>
          </div>
        <!--   <div class="col-md-3">
            <h2 class="footer-heading mb-4">Subscribe Newsletter</h2>
            <form action="#" method="post">
              <div class="input-group mb-3">
                <input type="text" class="form-control border-secondary text-white bg-transparent" placeholder="Enter Email" aria-label="Enter Email" aria-describedby="button-addon2">
                <div class="input-group-append">
                  <button class="btn btn-primary text-white" type="button" id="button-addon2">Send</button>
                </div>
              </div>
            </form>
          </div> -->
        </div>
        <div class="row pt-5 mt-5 text-center">
          <div class="col-md-12">
            <div class="border-top pt-5">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made with <i class="icon-heart text-danger" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
            </div>
          </div>

        </div>
      </div>
    </footer>

  </div> <!-- .site-wrap -->

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/jquery.easing.1.3.js"></script>
  <script src="js/aos.js"></script>
  <script src="js/jquery.fancybox.min.js"></script>
  <script src="js/jquery.sticky.js"></script>

  <script src="js/typed.js"></script>
            <script>
            var typed = new Typed('.typed-words', {
            strings: ["Human Centered Computing","Humanoid Robots","Affective Computing","Interaction's"],
            typeSpeed: 80,
            backSpeed: 80,
            backDelay: 4000,
            startDelay: 1000,
            loop: true,
            showCursor: true
            });
            </script>

  <script src="js/main.js"></script>


  <script>
    filterSelection("all")
    function filterSelection(c) {
      var x, i;
      // console.log("x.......");
      x = document.getElementsByClassName("filterDiv");
      // console.log(typeof(x));
      // x = x.map(function(d) { return d.replace('col-md-6 col-lg-4-p.', ''); });

      // console.log(typeof(x[0].className));
      // console.log(".......");
      if (c == "all") c = "";
      for (i = 0; i < x.length; i++) {
        removeClass(x[i], "show");
        if (x[i].className.indexOf(c) > -1) addClass(x[i], "show");
      }
    }

    function addClass(element, name) {
      var i, arr1, arr2;
      arr1 = element.className.split(" ");
      // console.log("arr...");
      // arr1=arr1.slice(2,);
      // console.log(arr1);
      arr2 = name.split(" ");
      for (i = 0; i < arr2.length; i++) {
        if (arr1.indexOf(arr2[i]) == -1) {element.className += " " + arr2[i];}
      }
    }

    function removeClass(element, name) {
      var i, arr1, arr2;
      arr1 = element.className.split(" ");
      arr2 = name.split(" ");
      for (i = 0; i < arr2.length; i++) {
        while (arr1.indexOf(arr2[i]) > -1) {
          arr1.splice(arr1.indexOf(arr2[i]), 1);     
        }
      }
      element.className = arr1.join(" ");
    }

    var btnContainer = document.getElementById("myBtnContainer");
    var btns = btnContainer.getElementsByClassName("btn_pr");
    for (var i = 0; i < btns.length; i++) {
      btns[i].addEventListener("click", function(){
        var current = document.querySelectorAll(".btn_pr.active");
        if (current.length > 0) {
          console.log(current);
          current[0].className = current[0].className.replace(" active", "");
        }
        // current[0].className = current[0].className.replace(" active", "");
        this.className += " active";
      });
    }
  </script>



  </body>
</html>
